@article{wolters2024memory,
  title={Memory Is All You Need: An Overview of Compute-in-Memory Architectures for Accelerating Large Language Model Inference},
  author={Wolters, Christopher and Yang, Xiaoxuan and Schlichtmann, Ulf and Suzumura, Toyotaro},
  journal={arXiv preprint arXiv:2406.08413},
  year={2024}
}

@misc{andrulis2024cimloopflexibleaccuratefast,
      title={CiMLoop: A Flexible, Accurate, and Fast Compute-In-Memory Modeling Tool}, 
      author={Tanner Andrulis and Joel S. Emer and Vivienne Sze},
      year={2024},
      eprint={2405.07259},
      archivePrefix={arXiv},
      primaryClass={cs.AR},
      url={https://arxiv.org/abs/2405.07259}, 
}

@inproceedings{NIPS2017_3f5ee243,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@ARTICLE{9512855,
  author={Yu, Shimeng and Jiang, Hongwu and Huang, Shanshi and Peng, Xiaochen and Lu, Anni},
  journal={IEEE Circuits and Systems Magazine}, 
  title={Compute-in-Memory Chips for Deep Learning: Recent Trends and Prospects}, 
  year={2021},
  volume={21},
  number={3},
  pages={31-56},
  keywords={Deep learning;Training data;Performance evaluation;Three-dimensional displays;Nonvolatile memory;Resistive RAM;Throughput;Common information Model;Memory management;Hardware acceleration;Neural networks},
  doi={10.1109/MCAS.2021.3092533}
}

@article{touvron2023llamatwo,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year={2018},
  publisher={San Francisco, CA, USA}
}

@article{wan2022compute,
  title={A compute-in-memory chip based on resistive random-access memory},
  author={Wan, Weier and Kubendran, Rajkumar and Schaefer, Clemens and Eryilmaz, Sukru Burc and Zhang, Wenqiang and Wu, Dabin and Deiss, Stephen and Raina, Priyanka and Qian, He and Gao, Bin and others},
  journal={Nature},
  volume={608},
  number={7923},
  pages={504--512},
  year={2022},
  publisher={Nature Publishing Group}
}

@article{kim2021colonnade,
  title={Colonnade: A reconfigurable SRAM-based digital bit-serial compute-in-memory macro for processing neural networks},
  author={Kim, Hyunjoon and Yoo, Taegeun and Kim, Tony Tae-Hyoung and Kim, Bongjin},
  journal={IEEE Journal of Solid-State Circuits},
  volume={56},
  number={7},
  pages={2221--2233},
  year={2021},
  publisher={IEEE}
}

@article{sinangil20207,
  title={A 7-nm compute-in-memory SRAM macro supporting multi-bit input, weight and output and achieving 351 TOPS/W and 372.4 GOPS},
  author={Sinangil, Mahmut E and Erbagci, Burak and Naous, Rawan and Akarvardar, Kerem and Sun, Dar and Khwa, Win-San and Liao, Hung-Jen and Wang, Yih and Chang, Jonathan},
  journal={IEEE Journal of Solid-State Circuits},
  volume={56},
  number={1},
  pages={188--198},
  year={2020},
  publisher={IEEE}
}

@ARTICLE{2018arXiv181004805D,
       author = {{Devlin}, Jacob and {Chang}, Ming-Wei and {Lee}, Kenton and {Toutanova}, Kristina},
        title = "{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language},
         year = 2018,
        month = oct,
          eid = {arXiv:1810.04805},
        pages = {arXiv:1810.04805},
          doi = {10.48550/arXiv.1810.04805},
archivePrefix = {arXiv},
       eprint = {1810.04805},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018arXiv181004805D},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@INPROCEEDINGS{8695666,
  author={Parashar, Angshuman and Raina, Priyanka and Shao, Yakun Sophia and Chen, Yu-Hsin and Ying, Victor A. and Mukkara, Anurag and Venkatesan, Rangharajan and Khailany, Brucek and Keckler, Stephen W. and Emer, Joel},
  booktitle={2019 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)}, 
  title={Timeloop: A Systematic Approach to DNN Accelerator Evaluation}, 
  year={2019},
  volume={},
  number={},
  pages={304-315},
  keywords={Hardware;Neural networks;Space exploration;Systematics;Accelerator architectures;Computational modeling;modeling;accelerator architecture;deep neural networks;neural network dataflows},
  doi={10.1109/ISPASS.2019.00042}}

@INPROCEEDINGS{8942149,
  author={Wu, Yannan Nellie and Emer, Joel S. and Sze, Vivienne},
  booktitle={2019 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)}, 
  title={Accelergy: An Architecture-Level Energy Estimation Methodology for Accelerator Designs}, 
  year={2019},
  volume={},
  number={},
  pages={1-8},
  keywords={Program processors;Electric breakdown;Neural networks;Estimation;Hardware;Energy efficiency;Compounds},
  doi={10.1109/ICCAD45719.2019.8942149}}

@article{abdin2024phi,
  title={Phi-3 technical report: A highly capable language model locally on your phone},
  author={Abdin, Marah and Jacobs, Sam Ade and Awan, Ammar Ahmad and Aneja, Jyoti and Awadallah, Ahmed and Awadalla, Hany and Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Behl, Harkirat and others},
  journal={arXiv preprint arXiv:2404.14219},
  year={2024}
}

@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@article{fournier2023practical,
  title={A practical survey on faster and lighter transformers},
  author={Fournier, Quentin and Caron, Ga{\'e}tan Marceau and Aloise, Daniel},
  journal={ACM Computing Surveys},
  volume={55},
  number={14s},
  pages={1--40},
  year={2023},
  publisher={ACM New York, NY}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{ding2024longrope,
  title={Longrope: Extending llm context window beyond 2 million tokens},
  author={Ding, Yiran and Zhang, Li Lyna and Zhang, Chengruidong and Xu, Yuanyuan and Shang, Ning and Xu, Jiahang and Yang, Fan and Yang, Mao},
  journal={arXiv preprint arXiv:2402.13753},
  year={2024}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}